{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f5067",
   "metadata": {},
   "outputs": [],
   "source": [
    "CITY_GRAPH_FILES = {\n",
    "    \"Boston\": [\n",
    "        \"Boston_Massachusetts_USA.graphml\",\n",
    "    ],\n",
    "    \"Chicago\": [\n",
    "        \"Chicago.graphml\",\n",
    "        \"Chicago_Illinois.graphml\",\n",
    "        \"Chicago_Illinois_USA.graphml\",\n",
    "    ],\n",
    "    \"Dallas\": [\n",
    "        \"Dallas_Texas_USA.graphml\",\n",
    "    ],\n",
    "    \"Phoenix\": [\n",
    "        \"Phoenix_Arizona_USA.graphml\",\n",
    "    ],\n",
    "    \"Pittsburgh\": [\n",
    "        \"Pittsburgh.graphml\",\n",
    "        \"Pittsburgh_Pennsylvania.graphml\",\n",
    "        \"Pittsburgh_Pennsylvania_USA.graphml\",\n",
    "        \"Pittsburgh_Allegheny_County_Pennsylvania_United_States.graphml\",\n",
    "    ],\n",
    "    \"San Francisco\": [\n",
    "        \"San_Francisco_California_USA.graphml\",\n",
    "    ],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b814f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading graph for Boston: Boston.graphml\n",
      "Loading graph for Chicago: Chicago_Illinois.graphml\n",
      "Loading graph for Dallas: Dallas_Texas_USA.graphml\n",
      "Loading graph for Phoenix: Phoenix_Arizona_USA.graphml\n",
      "Loading graph for Pittsburgh: Pittsburgh.graphml\n",
      "Loading graph for San Francisco: San_Francisco_California_USA.graphml\n",
      "\n",
      "Baseline Network Statistics\n",
      "   nodes  edges   density  avg_degree      aspl  clustering  assortativity  \\\n",
      "0  11408  16618  0.000255    2.913394  1.333333    0.048467       0.127018   \n",
      "1  29362  48731  0.000113    3.319324  1.000000    0.026053       0.197730   \n",
      "2  36544  56723  0.000085    3.104367  1.333333    0.048720       0.176540   \n",
      "3  48456  67262  0.000057    2.776209  1.000000    0.050153       0.093789   \n",
      "4   9122  12988  0.000312    2.847621  1.333333    0.040665       0.101622   \n",
      "5  10012  16393  0.000327    3.274670  1.500000    0.054249       0.216187   \n",
      "\n",
      "   bridge_fraction           city  \n",
      "0         0.103803         Boston  \n",
      "1         0.047731        Chicago  \n",
      "2         0.068385         Dallas  \n",
      "3         0.150486        Phoenix  \n",
      "4         0.148060     Pittsburgh  \n",
      "5         0.059050  San Francisco  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# GLOBAL SETTINGS\n",
    "# -----------------------------\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "ox.settings.log_console = False\n",
    "ox.settings.use_cache = False   # we are NOT using Overpass at all\n",
    "\n",
    "GRAPH_DIR = \"/Users/surabhiraghavan/Documents/Sem 3/Network Science and Analysis/urban-mobility/backend/graphs\"\n",
    "\n",
    "# -----------------------------\n",
    "# GRAPH FILE MAPPING\n",
    "# -----------------------------\n",
    "CITY_GRAPH_FILES = {\n",
    "    \"Boston\": [\n",
    "        \"Boston.graphml\",\n",
    "        \"Boston_Massachusetts_USA.graphml\",\n",
    "        \"Boston_Suffolk_County_Massachusetts_United_States.graphml\",\n",
    "    ],\n",
    "    \"Chicago\": [\n",
    "        \"Chicago.graphml\",\n",
    "        \"Chicago_Illinois.graphml\",\n",
    "        \"Chicago_Illinois_USA.graphml\",\n",
    "    ],\n",
    "    \"Dallas\": [\n",
    "        \"Dallas_Texas_USA.graphml\",\n",
    "    ],\n",
    "    \"Phoenix\": [\n",
    "        \"Phoenix_Arizona_USA.graphml\",\n",
    "    ],\n",
    "    \"Pittsburgh\": [\n",
    "        \"Pittsburgh.graphml\",\n",
    "        \"Pittsburgh_Pennsylvania.graphml\",\n",
    "        \"Pittsburgh_Pennsylvania_USA.graphml\",\n",
    "        \"Pittsburgh_Allegheny_County_Pennsylvania_United_States.graphml\",\n",
    "    ],\n",
    "    \"San Francisco\": [\n",
    "        \"San_Francisco_California_USA.graphml\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# GRAPH LOADER (GRAPHML ONLY)\n",
    "# -----------------------------\n",
    "def load_city_graph(city):\n",
    "    city_key = city.lower().replace(\" \", \"_\")\n",
    "\n",
    "    candidates = []\n",
    "    for fname in os.listdir(GRAPH_DIR):\n",
    "        if not fname.endswith(\".graphml\"):\n",
    "            continue\n",
    "\n",
    "        f_lower = fname.lower()\n",
    "\n",
    "        # must contain city name\n",
    "        if city_key not in f_lower:\n",
    "            continue\n",
    "\n",
    "        # exclude known false positives\n",
    "        if \"county\" in f_lower and city_key not in f_lower.split(\"_county\")[0]:\n",
    "            continue\n",
    "\n",
    "        candidates.append(fname)\n",
    "\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No GraphML file found for {city}\")\n",
    "\n",
    "    # Prefer shorter, cleaner filenames\n",
    "    candidates.sort(key=len)\n",
    "    chosen = candidates[0]\n",
    "    path = os.path.join(GRAPH_DIR, chosen)\n",
    "\n",
    "    print(f\"Loading graph for {city}: {chosen}\")\n",
    "    G = ox.load_graphml(path)\n",
    "\n",
    "    # ensure projected\n",
    "    if \"crs\" not in G.graph or G.graph[\"crs\"] is None:\n",
    "        G = ox.project_graph(G)\n",
    "\n",
    "    # ensure travel time\n",
    "    u, v, k, data = list(G.edges(keys=True, data=True))[0]\n",
    "    if \"travel_time\" not in data:\n",
    "        G = ox.add_edge_speeds(G)\n",
    "        G = ox.add_edge_travel_times(G)\n",
    "        ox.save_graphml(G, path)\n",
    "\n",
    "    return G\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# BASELINE NETWORK STATISTICS\n",
    "# -----------------------------\n",
    "def compute_baseline_stats(G):\n",
    "    Gu = nx.Graph(G)\n",
    "\n",
    "    stats = {}\n",
    "    stats[\"nodes\"] = Gu.number_of_nodes()\n",
    "    stats[\"edges\"] = Gu.number_of_edges()\n",
    "    stats[\"density\"] = nx.density(Gu)\n",
    "\n",
    "    degrees = np.array([d for _, d in Gu.degree()])\n",
    "    stats[\"avg_degree\"] = degrees.mean()\n",
    "\n",
    "    # approximate ASPL (sampled)\n",
    "    sample = random.sample(list(Gu.nodes()), min(500, Gu.number_of_nodes()))\n",
    "    subG = Gu.subgraph(sample)\n",
    "\n",
    "    # take largest connected component\n",
    "    if not nx.is_connected(subG):\n",
    "        largest_cc = max(nx.connected_components(subG), key=len)\n",
    "        subG = subG.subgraph(largest_cc)\n",
    "\n",
    "    stats[\"aspl\"] = nx.average_shortest_path_length(subG)\n",
    "\n",
    "\n",
    "    stats[\"clustering\"] = nx.average_clustering(Gu)\n",
    "    stats[\"assortativity\"] = nx.degree_assortativity_coefficient(Gu)\n",
    "\n",
    "    bridges = list(nx.bridges(Gu))\n",
    "    stats[\"bridge_fraction\"] = len(bridges) / Gu.number_of_edges()\n",
    "\n",
    "    return stats\n",
    "\n",
    "# -----------------------------\n",
    "# FAILURE MODELS\n",
    "# -----------------------------\n",
    "def select_edges(G, strategy, severity):\n",
    "    edges = list(G.edges(keys=True))\n",
    "    k = int(len(edges) * severity)\n",
    "\n",
    "    if strategy == \"random\":\n",
    "        return random.sample(edges, k)\n",
    "\n",
    "    if strategy == \"targeted\":\n",
    "        Gu = nx.Graph(G)\n",
    "        bc = nx.edge_betweenness_centrality(Gu, k=200, seed=RANDOM_SEED)\n",
    "        ranked = sorted(bc.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [(u, v, 0) for (u, v), _ in ranked[:k]]\n",
    "\n",
    "    raise ValueError(\"Unknown strategy\")\n",
    "\n",
    "# -----------------------------\n",
    "# OD PAIRS\n",
    "# -----------------------------\n",
    "def sample_od_pairs(G, n_pairs=200):\n",
    "    nodes = list(G.nodes())\n",
    "    return [tuple(random.sample(nodes, 2)) for _ in range(n_pairs)]\n",
    "\n",
    "# -----------------------------\n",
    "# SIMULATION ENGINE\n",
    "# -----------------------------\n",
    "def simulate_failure(G, strategy, severity, n_pairs=200):\n",
    "    G0 = G.copy()\n",
    "    Gf = G.copy()\n",
    "\n",
    "    Gf.remove_edges_from(select_edges(G, strategy, severity))\n",
    "\n",
    "    od_pairs = sample_od_pairs(G0, n_pairs)\n",
    "\n",
    "    ratios = []\n",
    "    disconnected = 0\n",
    "\n",
    "    for o, d in od_pairs:\n",
    "        try:\n",
    "            t0 = nx.shortest_path_length(G0, o, d, weight=\"travel_time\")\n",
    "            tf = nx.shortest_path_length(Gf, o, d, weight=\"travel_time\")\n",
    "            ratios.append(tf / t0)\n",
    "        except nx.NetworkXNoPath:\n",
    "            disconnected += 1\n",
    "\n",
    "    avg_ratio = np.mean(ratios) if ratios else np.inf\n",
    "    pct_disconnected = disconnected / n_pairs\n",
    "\n",
    "    resilience = np.exp(-avg_ratio) * (1 - pct_disconnected)\n",
    "\n",
    "    return avg_ratio, pct_disconnected, resilience\n",
    "\n",
    "# -----------------------------\n",
    "# LOAD ALL GRAPHS\n",
    "# -----------------------------\n",
    "cities = list(CITY_GRAPH_FILES.keys())\n",
    "graphs = {}\n",
    "baseline_rows = []\n",
    "\n",
    "for city in cities:\n",
    "    G = load_city_graph(city)\n",
    "    graphs[city] = G\n",
    "\n",
    "    row = compute_baseline_stats(G)\n",
    "    row[\"city\"] = city\n",
    "    baseline_rows.append(row)\n",
    "\n",
    "baseline_df = pd.DataFrame(baseline_rows)\n",
    "baseline_df.to_csv(\"baseline_network_stats.csv\", index=False)\n",
    "\n",
    "print(\"\\nBaseline Network Statistics\")\n",
    "print(baseline_df)\n",
    "\n",
    "# -----------------------------\n",
    "# RUN SIMULATIONS\n",
    "# -----------------------------\n",
    "severities = [0.05, 0.1, 0.2, 0.3]\n",
    "scenarios = [\"random\", \"targeted\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for city, G in graphs.items():\n",
    "    for scenario in scenarios:\n",
    "        for s in severities:\n",
    "            avg_ratio, pct_disc, res = simulate_failure(G, scenario, s)\n",
    "            results.append({\n",
    "                \"city\": city,\n",
    "                \"scenario\": scenario,\n",
    "                \"severity\": s,\n",
    "                \"avg_ratio\": avg_ratio,\n",
    "                \"pct_disconnected\": pct_disc,\n",
    "                \"resilience\": res\n",
    "            })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"simulation_results.csv\", index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# TABLE: SCENARIO SUMMARY\n",
    "# -----------------------------\n",
    "scenario_summary = (\n",
    "    results_df\n",
    "    .groupby(\"scenario\")[\"resilience\"]\n",
    "    .agg([\"mean\", \"std\"])\n",
    ")\n",
    "\n",
    "print(\"\\nScenario-wise Resilience\")\n",
    "print(scenario_summary)\n",
    "\n",
    "# -----------------------------\n",
    "# PLOT: ROBUSTNESS CURVES\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "for city in cities:\n",
    "    vals = results_df[\n",
    "        (results_df.city == city) &\n",
    "        (results_df.scenario == \"random\")\n",
    "    ].sort_values(\"severity\")\n",
    "\n",
    "    plt.plot(vals.severity, vals.resilience, marker=\"o\", label=city)\n",
    "\n",
    "plt.xlabel(\"Severity (fraction removed)\")\n",
    "plt.ylabel(\"Resilience\")\n",
    "plt.title(\"Resilience vs Severity (Random Failure)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# PLOT: DISCONNECTED OD PAIRS\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "for city in cities:\n",
    "    vals = results_df[\n",
    "        (results_df.city == city) &\n",
    "        (results_df.scenario == \"random\")\n",
    "    ].sort_values(\"severity\")\n",
    "\n",
    "    plt.plot(vals.severity, vals.pct_disconnected, marker=\"o\", label=city)\n",
    "\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.ylabel(\"Fraction Disconnected OD Pairs\")\n",
    "plt.title(\"Connectivity Collapse vs Severity\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# RANDOM VS TARGETED (CASE STUDY)\n",
    "# -----------------------------\n",
    "city = \"Pittsburgh\"\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for scenario in scenarios:\n",
    "    vals = results_df[\n",
    "        (results_df.city == city) &\n",
    "        (results_df.scenario == scenario)\n",
    "    ].sort_values(\"severity\")\n",
    "\n",
    "    plt.plot(vals.severity, vals.resilience, marker=\"o\", label=scenario)\n",
    "\n",
    "plt.xlabel(\"Severity\")\n",
    "plt.ylabel(\"Resilience\")\n",
    "plt.title(f\"{city}: Random vs Targeted Failure\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# FEATURE–RESILIENCE CORRELATION\n",
    "# -----------------------------\n",
    "merged = results_df.merge(baseline_df, on=\"city\")\n",
    "\n",
    "features = [\n",
    "    \"density\",\n",
    "    \"avg_degree\",\n",
    "    \"aspl\",\n",
    "    \"clustering\",\n",
    "    \"assortativity\",\n",
    "    \"bridge_fraction\"\n",
    "]\n",
    "\n",
    "corrs = {\n",
    "    f: merged[f].corr(merged[\"resilience\"])\n",
    "    for f in features\n",
    "}\n",
    "\n",
    "corr_df = pd.DataFrame.from_dict(corrs, orient=\"index\", columns=[\"correlation\"])\n",
    "corr_df.to_csv(\"feature_resilience_correlations.csv\")\n",
    "\n",
    "print(\"\\nFeature–Resilience Correlations\")\n",
    "print(corr_df.sort_values(\"correlation\"))\n",
    "\n",
    "# -----------------------------\n",
    "# ASSORTATIVITY VS RESILIENCE\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "for city in cities:\n",
    "    x = baseline_df.loc[baseline_df.city == city, \"assortativity\"].values[0]\n",
    "    y = results_df.loc[results_df.city == city, \"resilience\"].mean()\n",
    "    plt.scatter(x, y, s=80)\n",
    "    plt.text(x, y, city, fontsize=9)\n",
    "\n",
    "plt.xlabel(\"Assortativity\")\n",
    "plt.ylabel(\"Mean Resilience\")\n",
    "plt.title(\"Assortativity vs Urban Resilience\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (Python 3.12)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
